
# Map Style Transfer with MapCGAN

## Overview
This project delves into the domain of image style transfer, a notable subtask within the broader field of image processing. Our focal objective is to accomplish map style transfer through a neural network model, which we have named MapCGAN. We meticulously selected over a thousand map images from Google Maps, forming our comprehensive dataset for training and testing purposes. 

Utilizing the CycleGAN framework, we have proficiently trained two generators that are capable of executing bidirectional mapping between satellite imagery and standard map representations. To augment the efficiency and effectiveness of our models, we have incorporated a Self Attention Mechanism and adopted the Learned Perceptual Image Patch Similarity (LPIPS) loss. This strategic choice addresses the limitations inherent in the traditional L2 loss methodology. Additionally, we innovated by adjusting the weight of the components within the Cycle Consistency Loss, specifically tailored to cater to transformations across different domains.

For the purpose of evaluation, we have applied the Structural Similarity Index (SSIM) and the Peak Signal-to-Noise Ratio (PSNR). These metrics serve to validate the quality and accuracy of the images generated by our model. Our findings reveal that, despite some minor alterations in the structural details of the maps post style transfer, there is a conspicuous transformation in terms of color and local texture, marking a significant shift in style.

## Branches
The project is organized into multiple branches, each encapsulating specific aspects of the research and development process:

### Master
The `master` branch amalgamates all improvements made during the project, including:
1. **Self-attention Mechanisms and SA_Blocks**: Enhances the generator by improving the understanding and global feature extraction from satellite images, facilitating precise reconstruction of local and global information.
2. **Cycle Consistency Loss Weight Adjustment**: Tailors the model for domains with varying amounts of information (e.g., satellite images vs. plain maps), promoting a more natural and appropriate reduction of information during the style transfer process.
3. **LPIPS Loss**: Substitutes the traditional L1 norm with LPIPS loss to align more closely with human perceptual similarity, thereby elevating the visual quality of image transformations.
4. **Training Enhancements**: Implements various training strategies, such as label smoothing, input noise addition, and frequency adjustment of discriminator and generator training, to stabilize the model training and elevate the quality of the output images.

### MapCGAN-gen
The `MapCGAN-gen` branch omits the generator modifications, focusing on other aspects of the model.

### MapCGAN-lpips
The `MapCGAN-lpips` branch removes the LPIPS loss, offering insights into the impact of this loss function on the overall model performance.

### MapCGAN-λadj
The `MapCGAN-λadj` branch does not incorporate the strategy of encouraging information loss, providing a comparative perspective on the model's behavior without this adjustment.

## Conclusion
This project represents a significant step forward in the field of map style transfer. Through innovative approaches and meticulous evaluation, we have demonstrated the potential of MapCGAN to revolutionize the way we perceive and interact with map data. We invite collaborators and researchers to explore our findings and contribute further to this exciting domain.



## Prerequisites
- Windows
- Python 3
- CPU or NVIDIA GPU + CUDA CuDNN

## Getting Started
### Installation

- Clone this repo:
```bash
git clone https://github.com/Jtianyue/MapCGAN.git
```

- Install [PyTorch](http://pytorch.org) and 0.4+ and other dependencies (e.g., torchvision, [visdom](https://github.com/facebookresearch/visdom) and [dominate](https://github.com/Knio/dominate)).
  - For pip users, please type the command `pip install -r requirements.txt`.
  - For Conda users, you can create a new Conda environment using `conda env create -f environment.yml`.
  - For Docker users, we provide the pre-built Docker image and Dockerfile. Please refer to our [Docker](docs/docker.md) page.
  - For Repl users, please click [![Run on Repl.it](https://repl.it/badge/github/junyanz/pytorch-CycleGAN-and-pix2pix)](https://repl.it/github/junyanz/pytorch-CycleGAN-and-pix2pix).

### CycleGAN train/test

- To view training results and loss plots, run `python -m visdom.server` and click the URL http://localhost:8097.
- To log training progress and test images to W&B dashboard, set the `--use_wandb` flag with train and test script
- Train a model:
```bash
#!./scripts/train_cyclegan.sh
python train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan
```

- Test the model:
```bash
#!./scripts/test_cyclegan.sh
python test.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan
```
- The test results will be saved to a html file here: `./results/maps_cyclegan/latest_test/index.html`.
